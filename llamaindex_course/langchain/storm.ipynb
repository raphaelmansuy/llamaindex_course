{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORM\n",
    "\n",
    "[STORM](https://arxiv.org/abs/2402.14207) is a research assistant designed by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.\n",
    "\n",
    "STORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:\n",
    "\n",
    "1. Creating an outline (planning) by querying similar topics helps improve coverage.\n",
    "2. Multi-perspective, grounded (in search) conversation simulation helps increase the reference count and information density. \n",
    "\n",
    "The control flow looks like the diagram below.\n",
    "\n",
    "![STORM diagram](./imgages/storm.png)\n",
    "\n",
    "STORM has a few main stages:\n",
    "\n",
    "1. Generate initial outline + Survey related subjects\n",
    "2. Identify distinct perspectives\n",
    "3. \"Interview subject matter experts\" (role-playing LLMs)\n",
    "4. Refine outline (using references)\n",
    "5. Write sections, then write article\n",
    "\n",
    "\n",
    "The expert interviews stage ocurrs between the role-playing article writer and a research expert. The \"expert\" is able to query external knowledge and respond to pointed questions, saving cited sources to a vectorstore so that the later refinement stages can synthesize the full article.\n",
    "\n",
    "There are a couple hyperparameters you can set to restrict the (potentially) infinite research breadth:\n",
    "\n",
    "N: Number of perspectives to survey / use (Steps 2->3)\n",
    "M: Max number of conversation turns in step (Step 3)\n",
    "\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain_community langchain_openai langgraph wikipedia  scikit-learn  langchain_fireworks\n",
    "# We use one or the other search engine below\n",
    "# %pip install -U duckduckgo tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to draw the pretty graph diagrams.\n",
    "# If you are on MacOS, you will need to run brew install graphviz before installing and update some environment flags\n",
    "# ! brew install graphviz\n",
    "# !CFLAGS=\"-I $(brew --prefix graphviz)/include\" LDFLAGS=\"-L $(brew --prefix graphviz)/lib\" pip install -U pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if os.environ.get(var):\n",
    "        return\n",
    "    os.environ[var] = getpass.getpass(var + \":\")\n",
    "\n",
    "\n",
    "# Set for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"STORM\"\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select LLMs\n",
    "\n",
    "We will have a faster LLM do most of the work, but a slower, long-context model to distill the conversations and write the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# Uncomment for a Fireworks model\n",
    "# fast_llm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", max_tokens=32_000)\n",
    "#long_context_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Initial Outline\n",
    "\n",
    "For many topics, your LLM may have an initial idea of the important and related topics. We can generate an initial\n",
    "outline to be refined after our research. Below, we will use our \"fast\" llm to generate the outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphaelmansuy/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
    "        ),\n",
    "        (\"user\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of million-plus token context window language models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Overview of million-plus token context window language models and their significance in natural language processing (NLP) tasks. Introduction to the Retrieval-Augmented Generation (RAG) framework.\n",
      "\n",
      "## Million-Plus Token Context Window Language Models\n",
      "\n",
      "Explanation of million-plus token context window language models, their architecture, training data, benefits, and challenges. Comparison with traditional language models.\n",
      "\n",
      "## Retrieval-Augmented Generation (RAG) Framework\n",
      "\n",
      "Overview of the RAG framework, its components, and how it combines retrieval and generation models for enhanced performance in question-answering and natural language understanding tasks.\n",
      "\n",
      "## Impact on RAG\n",
      "\n",
      "Analysis of the impact of million-plus token context window language models on the performance of RAG. Discussion on improvements in retrieval accuracy, generation quality, and overall effectiveness of the RAG framework with the integration of large context window models.\n"
     ]
    }
   ],
   "source": [
    "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Topics\n",
    "\n",
    "While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine.\n",
    "\n",
    "We will start our search by generating a list of related topics, sourced from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
    "\n",
    "Please list the as many subjects and urls as you can.\n",
    "\n",
    "Topic of interest: {topic}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(\n",
    "    RelatedSubjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['Language models', 'Retrieval-Augmented Generation (RAG)', 'Natural Language Processing', 'Machine Learning', 'Artificial Intelligence'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Perspectives\n",
    "\n",
    "From these related subjects, we can select representative Wikipedia editors as \"subject matter experts\" with distinct\n",
    "backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
    "    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\n",
    "\n",
    "    Wiki page outlines of related topics for inspiration:\n",
    "    {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata[\"categories\"])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
    "        :max_length\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Research Institution',\n",
       "   'name': 'Dr. Sarah Linguist',\n",
       "   'role': 'Language Model Expert',\n",
       "   'description': 'Dr. Sarah focuses on the technical aspects of language models, their architecture, training methodologies, and the impact of million-plus token context windows on the RAG (Retrieval-Augmented Generation) model.'},\n",
       "  {'affiliation': 'Tech Company',\n",
       "   'name': 'Alex AI',\n",
       "   'role': 'AI Engineer',\n",
       "   'description': 'Alex specializes in implementing language models in real-world applications. They will provide insights on how million-plus token context windows in language models affect the performance of RAG in practical settings.'},\n",
       "  {'affiliation': 'Academic Institution',\n",
       "   'name': 'Prof. NLP',\n",
       "   'role': 'Natural Language Processing Scholar',\n",
       "   'description': \"Prof. NLP's expertise lies in the intersection of NLP and machine learning. They will contribute insights on the historical development of language models and their implications for RAG.\"},\n",
       "  {'affiliation': 'Startup',\n",
       "   'name': 'Emily Innovate',\n",
       "   'role': 'LangChain Developer',\n",
       "   'description': 'Emily is a developer working with LangChain, a framework for large language models. She will discuss how LangChain can optimize the integration of million-plus token context window models in RAG applications.'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Now the true fun begins, each wikipedia writer is primed to role-play using the perspectives presented above. It will ask a series of questions of a second \"domain expert\" with access to a search engine. This generate content to generate a refined outline as well as an updated index of reference documents.\n",
    "\n",
    "\n",
    "### Interview State\n",
    "\n",
    "The conversation is cyclic, so we will construct it within its own graph. The State will contain messages, the reference docs, and the editor (with its own \"persona\") to make it easy to parallelize these conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dialog Roles\n",
    "\n",
    "The graph will have two participants: the wikipedia editor (`generate_question`), who asks questions based on its assigned role, and a domain expert (`gen_answer_chain), who uses a search engine to answer the questions as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, that's correct. I'm interested in understanding how the use of million-plus token context windows in language models like RAG affects their performance and capabilities. Can you provide insights on how such large context windows influence the training process and the overall effectiveness of the model in information retrieval and generation tasks?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "question[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer questions\n",
    "\n",
    "The `gen_answer_chain` first generates queries (query expansion) to answer the editor's question, then responds with citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
    "    )\n",
    "\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Queries' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gen_queries_chain\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)]}\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m \u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparsed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mqueries\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Queries' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "queries[\"parsed\"].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "\n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations\n",
    ").with_config(run_name=\"GenerateAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "# Tavily is typically a better search engine, but your free queries are limited\n",
    "# search_engine = TavilySearchResults(max_results=4)\n",
    "\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: RunnableConfig | None = None,\n",
    "    name: str = \"Subject Matter Expert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Queries' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m example_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gen_answer(\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mquestion[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)]}\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m example_answer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mgen_answer\u001b[0;34m(state, config, name, max_str_len)\u001b[0m\n\u001b[1;32m     11\u001b[0m swapped_state \u001b[38;5;241m=\u001b[39m swap_roles(state, name)  \u001b[38;5;66;03m# Convert all other AI messages\u001b[39;00m\n\u001b[1;32m     12\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gen_queries_chain\u001b[38;5;241m.\u001b[39mainvoke(swapped_state)\n\u001b[1;32m     13\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_engine\u001b[38;5;241m.\u001b[39mabatch(\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparsed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mqueries, config, return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m successful_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     res \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m query_results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     19\u001b[0m all_query_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m results \u001b[38;5;129;01min\u001b[39;00m successful_results \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m     21\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Queries' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the Interview Graph\n",
    "\n",
    "\n",
    "Now that we've defined the editor and domain expert, we can compose them in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject Matter Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Install pygraphviz to draw graphs: `pip install pygraphviz`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_png.py:74\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Feel free to comment out if you have\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# not installed pygraphviz\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m Image(\u001b[43minterview_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph.py:266\u001b[0m, in \u001b[0;36mGraph.draw_png\u001b[0;34m(self, output_file_path, fontname, labels)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_png\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m     output_file_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m     fontname: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m     labels: Optional[LabelsDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    263\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_png\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PngDrawer\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPngDrawer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfontname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/graph_png.py:76\u001b[0m, in \u001b[0;36mPngDrawer.draw\u001b[0;34m(self, graph, output_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpgv\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall pygraphviz to draw graphs: `pip install pygraphviz`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Create a directed graph\u001b[39;00m\n\u001b[1;32m     81\u001b[0m viz \u001b[38;5;241m=\u001b[39m pgv\u001b[38;5;241m.\u001b[39mAGraph(directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nodesep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, ranksep\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Install pygraphviz to draw graphs: `pip install pygraphviz`."
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Feel free to comment out if you have\n",
    "# not installed pygraphviz\n",
    "Image(interview_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "--  [AIMessage(content=\"Could you please explain how the use of million-plus token context windows in language models like RAG affects the model's performance in terms of retrieval and generation capabilities?\", name='Dr. Sarah Linguist')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Queries' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m final_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meditor\u001b[39m\u001b[38;5;124m\"\u001b[39m: perspectives\u001b[38;5;241m.\u001b[39meditors[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     ],\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m interview_graph\u001b[38;5;241m.\u001b[39mastream(initial_state):\n\u001b[1;32m     13\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(step))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4149\u001b[0m, in \u001b[0;36mRunnableBindingBase.astream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4143\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastream\u001b[39m(\n\u001b[1;32m   4144\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4145\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4146\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4148\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Output]:\n\u001b[0;32m-> 4149\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   4150\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4152\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4153\u001b[0m     ):\n\u001b[1;32m   4154\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:657\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput_stream\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matransform(\n\u001b[1;32m    658\u001b[0m     input_stream(),\n\u001b[1;32m    659\u001b[0m     config,\n\u001b[1;32m    660\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    661\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    663\u001b[0m ):\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:675\u001b[0m, in \u001b[0;36mPregel.atransform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21matransform\u001b[39m(\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28minput\u001b[39m: AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    674\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform_stream_with_config(\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    677\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atransform,\n\u001b[1;32m    678\u001b[0m         config,\n\u001b[1;32m    679\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    680\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    682\u001b[0m     ):\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1611\u001b[0m, in \u001b[0;36mRunnable._atransform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1611\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m             py_anext(iterator),  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   1614\u001b[0m         )\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1616\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m cast(Output, \u001b[38;5;28;01mawait\u001b[39;00m py_anext(iterator))\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:524\u001b[0m, in \u001b[0;36mPregel._atransform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    517\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    518\u001b[0m     futures,\n\u001b[1;32m    519\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    520\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    521\u001b[0m )\n\u001b[1;32m    523\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    527\u001b[0m _apply_writes(\n\u001b[1;32m    528\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    529\u001b[0m )\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4081\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4075\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4077\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4078\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4079\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4080\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   4082\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4083\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4084\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4085\u001b[0m     )\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2109\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2109\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2110\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2111\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m             patch_config(\n\u001b[1;32m   2113\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2114\u001b[0m             ),\n\u001b[1;32m   2115\u001b[0m         )\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3543\u001b[0m, in \u001b[0;36mRunnableLambda.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable asynchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3542\u001b[0m the_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafunc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m-> 3543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall_with_config(\n\u001b[1;32m   3544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ainvoke,\n\u001b[1;32m   3545\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, the_func),\n\u001b[1;32m   3547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3548\u001b[0m )\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1311\u001b[0m, in \u001b[0;36mRunnable._acall_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1307\u001b[0m coro \u001b[38;5;241m=\u001b[39m acall_func_with_variable_args(\n\u001b[1;32m   1308\u001b[0m     func, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1309\u001b[0m )\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_context(asyncio\u001b[38;5;241m.\u001b[39mcreate_task):\n\u001b[0;32m-> 1311\u001b[0m     output: Output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[0;32m~/Github/03-working/llamaindex_course/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3490\u001b[0m, in \u001b[0;36mRunnableLambda._ainvoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3490\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m acall_func_with_variable_args(\n\u001b[1;32m   3491\u001b[0m         cast(Callable, afunc), \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   3492\u001b[0m     )\n\u001b[1;32m   3493\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mgen_answer\u001b[0;34m(state, config, name, max_str_len)\u001b[0m\n\u001b[1;32m     11\u001b[0m swapped_state \u001b[38;5;241m=\u001b[39m swap_roles(state, name)  \u001b[38;5;66;03m# Convert all other AI messages\u001b[39;00m\n\u001b[1;32m     12\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gen_queries_chain\u001b[38;5;241m.\u001b[39mainvoke(swapped_state)\n\u001b[1;32m     13\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_engine\u001b[38;5;241m.\u001b[39mabatch(\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparsed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mqueries, config, return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m successful_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     res \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m query_results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     19\u001b[0m all_query_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]: res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m results \u001b[38;5;129;01min\u001b[39;00m successful_results \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m     21\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Queries' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"Subject Matter Expert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mfinal_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m()))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "At this point in STORM, we've conducted a large amount of research from different perspectives. It's time to refine the original outline based on these investigations. Below, create a chain using the LLM with a long context window to update the original outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_context_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m refine_outline_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     ]\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Using turbo preview since the context can get quite long\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m refine_outline_chain \u001b[38;5;241m=\u001b[39m refine_outline_prompt \u001b[38;5;241m|\u001b[39m \u001b[43mlong_context_llm\u001b[49m\u001b[38;5;241m.\u001b[39mwith_structured_output(\n\u001b[1;32m     22\u001b[0m     Outline\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'long_context_llm' is not defined"
     ]
    }
   ],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page. \\\n",
    "You need to make sure that the outline is comprehensive and specific. \\\n",
    "Topic you are writing about: {topic} \n",
    "\n",
    "Old outline:\n",
    "\n",
    "{old_outline}\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\nWrite the refined Wikipedia outline:\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'refine_outline_chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m refined_outline \u001b[38;5;241m=\u001b[39m \u001b[43mrefine_outline_chain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_topic,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mold_outline\u001b[39m\u001b[38;5;124m\"\u001b[39m: initial_outline\u001b[38;5;241m.\u001b[39mas_str,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversations\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m### \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m final_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m         ),\n\u001b[1;32m      8\u001b[0m     }\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'refine_outline_chain' is not defined"
     ]
    }
   ],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of million-plus token context window language models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Provides a brief overview of million-plus token context window language models and their relevance to Retrieval-Augmented Generation (RAG) systems, setting the stage for a deeper exploration of their impact.\n",
      "\n",
      "## Background\n",
      "\n",
      "A foundational section to understand the core concepts involved.\n",
      "\n",
      "### Million-Plus Token Context Window Language Models\n",
      "\n",
      "Explains what million-plus token context window language models are, including notable examples like Gemini 1.5, focusing on their architecture, training data, and the evolution of their applications.\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "Describes the RAG framework, its unique approach of combining retrieval and generation models for enhanced natural language processing, and its significance in the AI landscape.\n",
      "\n",
      "## Impact on RAG Systems\n",
      "\n",
      "Delves into the effects of million-plus token context window language models on RAG, highlighting both the challenges and opportunities presented.\n",
      "\n",
      "### Performance and Efficiency\n",
      "\n",
      "Discusses how large context window models influence RAG performance, including aspects of latency, computational demands, and overall efficiency.\n",
      "\n",
      "### Generation Quality and Diversity\n",
      "\n",
      "Explores the impact on generation quality, the potential for more accurate and diverse outputs, and how these models address data biases and factual accuracy.\n",
      "\n",
      "### Technical Challenges\n",
      "\n",
      "Identifies specific technical hurdles such as prompt template design, context length limitations, and similarity searches in vector databases, and how they affect RAG systems.\n",
      "\n",
      "### Opportunities and Advancements\n",
      "\n",
      "Outlines the new capabilities and improvements in agent interaction, information retrieval, and response relevance that these models bring to RAG systems.\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "Considers ongoing research and potential future developments in the integration of million-plus token context window language models with RAG systems, including speculation on emerging trends and technologies.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "Summarizes the key points discussed in the article, reaffirming the significant impact of million-plus token context window language models on RAG systems.\n"
     ]
    }
   ],
   "source": [
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Article\n",
    "\n",
    "Now it's time to generate the full article. We will first divide-and-conquer, so that each section can be tackled by an individual llm. Then we will prompt the long-form LLM to refine the finished article (since each section may use an inconsistent voice).\n",
    "\n",
    "#### Create Retriever\n",
    "\n",
    "The research process uncovers a large number of reference documents that we may want to query during the final article-writing process.\n",
    "\n",
    "First, create the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across requests if you want.\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In Retrieval Augmented Generation (RAG), a longer context augments our model with more information. For LLMs that power agents, such as chatbots, longer context means more tools and capabilities. When summarizing, longer context means more comprehensive summaries. There exist plenty of use-cases for LLMs that are unlocked by longer context lengths.', metadata={'id': '20454848-23ac-4649-b083-81980532a77b', 'source': 'https://www.anyscale.com/blog/fine-tuning-llms-for-longer-context-and-better-rag-systems'}),\n",
       " Document(page_content='By the way, the context limits differ among models: two Claude models offer a 100K token context window, which works out to about 75,000 words, which is much higher than most other LLMs. The ...', metadata={'id': '1ee2d2bb-8f8e-4a7e-b45e-608b0804fe4c', 'source': 'https://www.infoworld.com/article/3712227/what-is-rag-more-accurate-and-reliable-llms.html'}),\n",
       " Document(page_content='Figure 1: LLM response accuracy goes down when context needed to answer correctly is found in the middle of the context window. The problem gets worse with larger context models. The problem gets ...', metadata={'id': 'a41d69e6-62eb-4abd-90ad-0892a2836cba', 'source': 'https://medium.com/@jm_51428/long-context-window-models-vs-rag-a73c35a763f2'}),\n",
       " Document(page_content='To improve performance, we used retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date information. As a result of using RAG, the writing quality of the LLM improves substantially, which has implications for the practical useability of LLMs in clinical trial-related writing.', metadata={'id': 'e1af6e30-8c2b-495b-b572-ac6a29067a94', 'source': 'https://arxiv.org/abs/2402.16406'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What's a long context LLM anyway?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sections\n",
    "\n",
    "Now you can generate the sections using the indexed docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "\n",
    "\n",
    "class WikiSection(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Full content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "    citations: List[str] = Field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
    "        return (\n",
    "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "            + f\"\\n\\n{citations}\".strip()\n",
    "        )\n",
    "\n",
    "\n",
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
    "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"Write the full WikiSection for the {section} section.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt\n",
    "    | long_context_llm.with_structured_output(WikiSection)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Background\n",
      "\n",
      "To fully appreciate the impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG) systems, it's essential to first understand the foundational concepts that underpin these technologies. This background section provides a comprehensive overview of both million-plus token context window language models and RAG, setting the stage for a deeper exploration of their integration and subsequent impacts on artificial intelligence and natural language processing.\n",
      "\n",
      "### Million-Plus Token Context Window Language Models\n",
      "\n",
      "Million-plus token context window language models, such as Gemini 1.5, represent a significant leap forward in the field of language modeling. These models are designed to process and understand large swathes of text, sometimes exceeding a million tokens in a single pass. The ability to handle such vast amounts of information at once allows for a deeper understanding of context and nuance, which is crucial for generating coherent and relevant text outputs. The development of these models involves sophisticated architecture and extensive training data, pushing the boundaries of what's possible in natural language processing. Over time, the applications of these models have evolved, extending their utility beyond mere text generation to complex tasks like sentiment analysis, language translation, and more.\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "The Retrieval-Augmented Generation framework represents a novel approach in the realm of artificial intelligence, blending the strengths of both retrieval and generation models to enhance natural language processing capabilities. At its core, RAG leverages a two-step process: initially, it uses a query to retrieve relevant documents or data from a knowledge base; this information is then utilized to inform and guide the generation of responses by a language model. This method addresses the limitations of fixed context windows by converting text to vector embeddings, facilitating a dynamic and flexible interaction with a vast array of information. RAG's unique approach has cemented its significance in the AI landscape, offering a pathway to more accurate, informative, and contextually relevant text generation.\n"
     ]
    }
   ],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate final article\n",
    "\n",
    "Now we can rewrite the draft to appropriately group all the citations and maintain a consistent voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",\"\n",
    "            \" avoiding duplicates in the footer. Include URLs in the footer.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "The integration of million-plus token context window language models into Retrieval-Augmented Generation (RAG) systems marks a pivotal advancement in the field of artificial intelligence (AI) and natural language processing (NLP). This article delves into the background of both technologies, explores their convergence, and examines the profound effects of this integration on the capabilities and applications of AI-driven language models.\n",
      "\n",
      "## Contents\n",
      "\n",
      "1. [Background](#Background)\n",
      "    1. [Million-Plus Token Context Window Language Models](#Million-Plus-Token-Context-Window-Language-Models)\n",
      "    2. [Retrieval-Augmented Generation (RAG)](#Retrieval-Augmented-Generation-(RAG))\n",
      "2. [Integration of Million-Plus Token Context Window Models and RAG](#Integration-of-Million-Plus-Token-Context-Window-Models-and-RAG)\n",
      "3. [Impact on Natural Language Processing](#Impact-on-Natural-Language-Processing)\n",
      "4. [Applications](#Applications)\n",
      "5. [Challenges and Limitations](#Challenges-and-Limitations)\n",
      "6. [Future Directions](#Future-Directions)\n",
      "7. [Conclusion](#Conclusion)\n",
      "8. [References](#References)\n",
      "\n",
      "## Background\n",
      "\n",
      "### Million-Plus Token Context Window Language Models\n",
      "\n",
      "Million-plus token context window language models, exemplified by systems like Gemini 1.5, have revolutionized language modeling by their ability to process and interpret extensive texts, potentially exceeding a million tokens in a single analysis[1]. The capacity to manage such large volumes of data enables these models to grasp context and subtlety to a degree previously unattainable, enhancing their effectiveness in generating text that is coherent, relevant, and nuanced. The development of these models has been characterized by innovative architecture and the utilization of vast training datasets, pushing the envelope of natural language processing capabilities[2].\n",
      "\n",
      "### Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "RAG systems represent an innovative paradigm in AI, merging the strengths of retrieval-based and generative models to improve the quality and relevance of text generation[3]. By initially retrieving related documents or data in response to a query, and subsequently using this information to guide the generation process, RAG overcomes the limitations inherent in fixed context windows. This methodology allows for dynamic access to a broad range of information, significantly enhancing the model's ability to generate accurate, informative, and contextually appropriate responses[4].\n",
      "\n",
      "## Integration of Million-Plus Token Context Window Models and RAG\n",
      "\n",
      "The integration of million-plus token context window models with RAG systems has been a natural progression in the quest for more sophisticated NLP solutions. By combining the extensive contextual understanding afforded by large context window models with the dynamic, information-rich capabilities of RAG, researchers and developers have been able to create AI systems that exhibit unprecedented levels of understanding, coherence, and relevance in text generation[5].\n",
      "\n",
      "## Impact on Natural Language Processing\n",
      "\n",
      "The fusion of these technologies has had a significant impact on the field of NLP, leading to advancements in several key areas:\n",
      "- **Enhanced Understanding**: The combined system exhibits a deeper comprehension of both the immediate context and broader subject matter[6].\n",
      "- **Improved Coherence**: Generated text is more coherent over longer passages, maintaining consistency and relevance[7].\n",
      "- **Increased Relevance**: Outputs are more contextually relevant, drawing accurately from a wider range of sources[8].\n",
      "\n",
      "## Applications\n",
      "\n",
      "This technological convergence has broadened the applicability of NLP systems in numerous fields, including but not limited to:\n",
      "- **Automated Content Creation**: Generating written content that is both informative and contextually appropriate for various platforms[9].\n",
      "- **Customer Support**: Providing answers that are not only accurate but also tailored to the specific context of user inquiries[10].\n",
      "- **Research Assistance**: Assisting in literature review and data analysis by retrieving and synthesizing relevant information from vast databases[11].\n",
      "\n",
      "## Challenges and Limitations\n",
      "\n",
      "Despite their advancements, the integration of these technologies faces several challenges:\n",
      "- **Computational Resources**: The processing of million-plus tokens and the dynamic retrieval of relevant information require significant computational power[12].\n",
      "- **Data Privacy and Security**: Ensuring the confidentiality and integrity of the data accessed by these systems poses ongoing concerns[13].\n",
      "- **Bias and Fairness**: The potential for inheriting and amplifying biases from training data remains a critical issue to address[14].\n",
      "\n",
      "## Future Directions\n",
      "\n",
      "Future research is likely to focus on optimizing computational efficiency, enhancing the models' ability to understand and generate more diverse and nuanced text, and addressing ethical considerations associated with AI and NLP technologies[15].\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The integration of million-plus token context window language models with RAG systems represents a milestone in the evolution of natural language processing, offering enhanced capabilities that have significant implications across various applications. As these technologies continue to evolve, they promise to further transform the landscape of AI-driven language models.\n",
      "\n",
      "## References\n",
      "\n",
      "1. Gemini 1.5 Documentation. (n.d.).\n",
      "2. The Evolution of Language Models. (2022).\n",
      "3. Introduction to Retrieval-Augmented Generation. (2021).\n",
      "4. Leveraging Large Context Windows for NLP. (2023).\n",
      "5. Integrating Context Window Models with RAG. (2023).\n",
      "6. Deep Learning in NLP. (2020).\n",
      "7. Coherence in Text Generation. (2019).\n",
      "8. Contextual Relevance in AI. (2021).\n",
      "9. Applications of NLP in Content Creation. (2022).\n",
      "10. AI in Customer Support. (2023).\n",
      "11. NLP for Research Assistance. (2021).\n",
      "12. Computational Challenges in NLP. (2022).\n",
      "13. Data Privacy in AI Systems. (2020).\n",
      "14. Addressing Bias in AI. (2021).\n",
      "15. Future of NLP Technologies. (2023)."
     ]
    }
   ],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 6 main stages in sequence:\n",
    ".\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article\n",
    "3. Refine the outline based on the conversations\n",
    "4. Index the reference docs from the conversations\n",
    "5. Write the individual sections of the article\n",
    "6. Write the final wiki\n",
    "\n",
    "The state tracks the outputs of each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"Subject Matter Expert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here to parallelize the interviews\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(storm.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_research\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Overview of Groq, NVIDIA, Llamma.cpp, and their significance in the field of La\n",
      "conduct_interviews\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', sections=[Section(section_title='Introduction', description='Overview of Groq, NVIDIA, Llamma.cpp, and their significance in the field of La\n",
      "refine_outline\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference', sections=[Section(section_title='Introduction', description='An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\n",
      "index_references\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference', sections=[Section(section_title='Introduction', description='An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\n",
      "write_sections\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference', sections=[Section(section_title='Introduction', description='An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\n",
      "write_article\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference', sections=[Section(section_title='Introduction', description='An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\n",
      "__end__\n",
      "--  {'topic': 'Groq, NVIDIA, Llamma.cpp and the future of LLM Inference', 'outline': Outline(page_title='Groq, NVIDIA, Llamma.cpp and the Future of LLM Inference', sections=[Section(section_title='Introduction', description='An overview of the significance and roles of Groq, NVIDIA, and Llamma.cpp in th\n"
     ]
    }
   ],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render the Wiki\n",
    "\n",
    "Now we can render the final wiki page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Large Language Model (LLM) Inference Technologies\n",
       "\n",
       "### Contents\n",
       "1. [Introduction](#Introduction)\n",
       "2. [Groq's Advancements in LLM Inference](#Groqs-Advancements-in-LLM-Inference)\n",
       "3. [NVIDIA's Contributions to LLM Inference](#NVIDIAs-Contributions-to-LLM-Inference)\n",
       "   1. [Hardware Innovations](#Hardware-Innovations)\n",
       "   2. [Software Solutions](#Software-Solutions)\n",
       "   3. [Research and Development](#Research-and-Development)\n",
       "4. [Llamma.cpp: Accelerating LLM Inference](#Llammacpp-Accelerating-LLM-Inference)\n",
       "5. [The Future of LLM Inference](#The-Future-of-LLM-Inference)\n",
       "6. [References](#References)\n",
       "\n",
       "### Introduction\n",
       "\n",
       "The advent of million-plus token context window language models, such as Gemini 1.5, has significantly advanced the field of artificial intelligence, particularly in natural language processing (NLP). These models have expanded the capabilities of machine learning in understanding and generating text over vastly larger contexts than previously possible. This leap in technology has paved the way for transformative applications across various domains, including the integration into Retrieval-Augmented Generation (RAG) systems to produce more accurate and contextually rich responses. \n",
       "\n",
       "### Groq's Advancements in LLM Inference\n",
       "\n",
       "Groq has introduced the Groq Linear Processor Unit (LPU), a purpose-built hardware architecture for LLM inference. This innovation positions Groq as a leader in efficient and high-performance LLM processing by optimizing the hardware specifically for LLM tasks. The Groq LPU dramatically reduces latency and increases the throughput of LLM inferences, facilitating advancements in a wide range of applications, from natural language processing to broader artificial intelligence technologies[1].\n",
       "\n",
       "### NVIDIA's Contributions to LLM Inference\n",
       "\n",
       "NVIDIA has played a pivotal role in advancing LLM inference through its GPUs, optimized for AI and machine learning workloads, and specialized software frameworks. The company's GPU architecture and software solutions, such as the CUDA Deep Neural Network library (cuDNN) and the TensorRT inference optimizer, are designed to accelerate computational processes and improve LLM performance. NVIDIA's active participation in research and development further underscores its commitment to enhancing the capabilities of LLMs[1].\n",
       "\n",
       "#### Hardware Innovations\n",
       "\n",
       "NVIDIA's GPU architecture facilitates high throughput and parallel processing for LLM inference tasks, significantly reducing inference time and enabling complex models to be used in real-time applications.\n",
       "\n",
       "#### Software Solutions\n",
       "\n",
       "NVIDIA's suite of software tools, including cuDNN and TensorRT, optimizes LLM performance on its hardware, streamlining the deployment of LLMs by improving their efficiency and reducing latency.\n",
       "\n",
       "#### Research and Development\n",
       "\n",
       "NVIDIA collaborates with academic and industry partners to develop new techniques and models that push the boundaries of LLM technology, aiming to make LLMs more powerful and applicable across a broader range of tasks.\n",
       "\n",
       "### Llamma.cpp: Accelerating LLM Inference\n",
       "\n",
       "Llamma.cpp is a framework developed to enhance the speed and efficiency of LLM inference. By integrating specialized hardware, such as Groq's LPU, and optimizing for parallel processing, Llamma.cpp significantly accelerates computation times and reduces energy consumption. The framework supports million-plus token context window models, enabling applications requiring deep contextual understanding and extensive knowledge retrieval[1][2].\n",
       "\n",
       "### The Future of LLM Inference\n",
       "\n",
       "The future of LLM inference is poised for transformative changes with advances in purpose-built hardware architectures like Groq's LPU. These innovations promise to enhance the speed and efficiency of LLM processing, leading to more interactive, capable, and integrated AI applications. The potential for advanced hardware and sophisticated LLMs to enable near-instantaneous processing of complex queries and interactions opens new avenues for research and application in various fields, suggesting a future where AI is seamlessly integrated into society[1][2].\n",
       "\n",
       "### References\n",
       "\n",
       "[1] \"Groq's LPU: Advancing LLM Inference Efficiency,\" Prompt Engineering. https://promptengineering.org/groqs-lpu-advancing-llm-inference-efficiency/\n",
       "\n",
       "[2] \"The Speed of Thought: Harnessing the Fastest LLM with Groq's LPU,\" Medium. https://medium.com/@anasdavoodtk1/the-speed-of-thought-harnessing-the-fastest-llm-with-groqs-lpu-11bb00864e9c"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# We will down-header the sections to create less confusion in this notebook\n",
    "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
